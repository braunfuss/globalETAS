{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETAS: Alaska 2018\n",
    "#### 23 January 2018  09:31:42 UTC m=7.9\n",
    "\n",
    "2018-11-30 17:29:28 (UTC)\n",
    "Location\n",
    "61.340°N 149.937°W\n",
    "\n",
    "lon: -149.937\n",
    "lat: 61.340\n",
    "Depth: 40.9 km\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed while loading urllib and/or urllib. maybe python 3.x?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myoder/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import datetime as dtm\n",
    "import matplotlib.dates as mpd\n",
    "import pytz\n",
    "tzutc = pytz.timezone('UTC')\n",
    "\n",
    "#import operator\n",
    "import math\n",
    "import random\n",
    "import numpy\n",
    "import scipy\n",
    "import scipy.optimize as spo\n",
    "from scipy import interpolate\n",
    "import itertools\n",
    "import sys\n",
    "#import scipy.optimize as spo\n",
    "import os\n",
    "import operator\n",
    "#from PIL import Image as ipp\n",
    "import multiprocessing as mpp\n",
    "#\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import json\n",
    "import pickle\n",
    "#\n",
    "#from sklearn.neighbors import NearestNeighbors\n",
    "import sklearn\n",
    "import sklearn.neighbors\n",
    "#\n",
    "import geopy.distance\n",
    "#from geopy.distance import vincenty\n",
    "#from geopy.distance import great_circle\n",
    "#\n",
    "#import shapely.geometry as sgp\n",
    "#\n",
    "from mpl_toolkits.basemap import Basemap as Basemap\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from geographiclib.geodesic import Geodesic as ggp\n",
    "#\n",
    "#import ANSStools as atp\n",
    "from yodiipy import ANSStools as atp\n",
    "#\n",
    "import contours2kml\n",
    "import globalETAS as gep\n",
    "import global_etas_auto as ggep\n",
    "from eq_params import *\n",
    "#\n",
    "from nepal_figs import *\n",
    "import optimizers\n",
    "#\n",
    "import random\n",
    "import geopy\n",
    "\n",
    "#\n",
    "# on a fresh linux install... (can we script this?)\n",
    "# stuff we have to do besides just clone this:\n",
    "# pip install geopy\n",
    "# conda install basemap\n",
    "# pip install geographiclib\n",
    "# conda install -c ioos rtree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# event was some time on the 24th or maybe late the 23rd. this, plus defaults, should find the event:\n",
    "#\n",
    "#Depth: 40.9 km\n",
    "    \n",
    "Lr_factor = 10.\n",
    "#\n",
    "# newest event:\n",
    "t0 = dtm.datetime(2018, 11, 30, 17, 29, 28, tzinfo=pytz.timezone('UTC'))\n",
    "to_dt = dtm.datetime.now(pytz.timezone('UTC'))\n",
    "#to_dt = t0 + dtm.timedelta(days=15)\n",
    "#\n",
    "t_ms = t0\n",
    "#Location\n",
    "lon0 = -149.937\n",
    "lat0 = 61.340 \n",
    "#\n",
    "m0 = 7.0\n",
    "d_lat = 7.\n",
    "d_lon = 7.\n",
    "#\n",
    "lats = [lat0-d_lat, lat0+d_lat]\n",
    "lons = [lon0-d_lon, lon0+d_lon]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etas_name: Alaska, 23 January 2018\n",
      "f_path: /home/myoder/Dropbox/Research/etas/Alaska_2018_11_30/etas_2018-11-30 20:57:20.377673+00:00\n",
      "f_root: etas_Alaska_20181130\n"
     ]
    }
   ],
   "source": [
    "#etas = ggep.auto_etas(to_dt=to_dt, Lr_factor=Lr_factor, dt_0=5)\n",
    "#italy_prams = {'do_recarray': True, 'D_fract': 1.5,\n",
    "#                't_0':dtm.datetime(1990, 1, 1, 0, 0, tzinfo=pytz.timezone('UTC')),\n",
    "#                't_now':to_dt, \n",
    "#                'lats': [42.,43.5], 'p': 1.1, 'b1': 1.0, 'mc': 2.5, 'q': 1.5,\n",
    "#                'lons': [12.,15.], 'dmstar': 1.0, 'b2': 1.5, 'd_tau': 2.28,\n",
    "#                'incat': None, 'fit_factor': 2.0, 'd_lambda': 1.76}\n",
    "cat_len_days = 3650\n",
    "eq_prams = {'do_recarray': True, 'D_fract': 1.5,\n",
    "               't_0':dtm.datetime(1990, 1, 1, 0, 0, tzinfo=pytz.timezone('UTC')),\n",
    "               't_now':to_dt, 'cat_len':3650,\n",
    "               'd_lat': .2, 'd_lon':.2, 'mc':2.0,\n",
    "               'lats': lats, 'p': 1.1, 'b1': 1.0, 'q': 1.5,\n",
    "               'lons': lons, 'dmstar': 1.0, 'b2': 1.5, 'd_tau': 2.28,\n",
    "               'incat': None, 'fit_factor': 2.0, 'd_lambda': 1.76,\n",
    "               'etas_range_factor': 25.0, 'etas_range_padding':3.}\n",
    "\n",
    "#nz_cat0 = test_cat = atp.cat_from_geonet(lats=lats, lons=lons, m_c=2.5, date_from=dtm.datetime(2000,1,1,tzinfo=atp.tzutc),\n",
    "#                              date_to=dtm.datetime.now(atp.tzutc))\n",
    "#nz_cat = gep.make_ETAS_catalog_mpp(incat=nz_cat0)\n",
    "#\n",
    "# TODO: integrate this directly into the globalETAS() class...\n",
    "my_cat = None\n",
    "#my_cat = atp.cat_from_anss_and_usgs(lons=lons, lats=lats, mc=eq_prams['mc'], cat_len_days=cat_len_days, \n",
    "#                            Nmax=None, rec_array=True)\n",
    "# date_range=['1990-1-1', '{}-{}-{} 23:59:59'.format(to_dt.year, to_dt.month, to_dt.day)],\n",
    "if not my_cat is None:\n",
    "    my_cat = gep.make_ETAS_catalog_mpp(incat=my_cat)\n",
    "#\n",
    "etas_name = 'Alaska, 23 January 2018'\n",
    "f_path = '/home/myoder/Dropbox/Research/etas/Alaska_2018_11_30/etas_{}'.format(eq_prams['t_now'])\n",
    "f_root = 'etas_Alaska_20181130'\n",
    "\n",
    "print('etas_name: {}'.format(etas_name))\n",
    "print('f_path: {}'.format(f_path))\n",
    "print('f_root: {}'.format(f_root))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run (not-)ETAS:\n",
    "- Note: we might have compiled a composite catalog from ANSS and USGS above. decide if we really want to do that.\n",
    "  - To use just the ANSS catalog, pass catalog=None, or set my_cat=None before we execute. For domestic events, this is probably the better bet, but only by a little bit. For overseas events, ANSS can take weeks to update, so using the USGS concatenation is a good thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin globalETAS.__init()__\n",
      "Overriding t_0 (etas catalog start date/time) for ETAS calculations. using catalog start, t_0 = t_now - catlen (3650.000000) = 2008-12-02 20:57:20.377673+00:00\n",
      "fetch and process catalog for dates: [datetime.datetime(2008, 12, 2, 20, 57, 20, 377673, tzinfo=<UTC>), datetime.datetime(2018, 11, 30, 20, 57, 20, 377673, tzinfo=<UTC>)]\n",
      "etas_prams:  {'do_recarray': False, 'b2': 1.5, 'b1': 1.0, 'dmstar': 1.0, 'q': 1.5, 'p': 1.1, 'fit_factor': 1.0, 'd_tau': 2.28, 'd_lambda': 1.76, 'D_fract': 1.5, 'date_range': [datetime.datetime(2008, 12, 2, 20, 57, 20, 377673, tzinfo=<UTC>), datetime.datetime(2018, 11, 30, 20, 57, 20, 377673, tzinfo=<UTC>)], 'mc': 2.0, 'lons': [-156.937, -142.937], 'lats': [54.34, 68.34], 'incat': None}\n",
      "data handle fetched...\n",
      " no file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myoder/anaconda3/lib/python3.6/site-packages/numpy/core/records.py:839: FutureWarning: fromrecords expected a list of tuples, may have received a list of lists instead. In the future that will raise an error\n",
      "  return fromrecords(obj, dtype=dtype, shape=shape, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results fetched.\n",
      "catalog fetched and processed.\n",
      "ETAS over etas_cat_range/xyz_range:  ([0, 11375], None)\n",
      "make_etas():\n",
      "etas_mpp worker xyz_range:  [0, 612]\n",
      "begin globalETAS.__init()__\n",
      "Overriding t_0 (etas catalog start date/time) for ETAS calculations. using catalog start, t_0 = t_now - catlen (3650.000000) = 2008-12-02 20:57:20.377673+00:00\n",
      "ETAS over etas_cat_range/xyz_range:  ([0, 11375], [0, 612])\n",
      "etas_mpp worker xyz_range:  [612, 1225]\n",
      "begin globalETAS.__init()__\n",
      "Overriding t_0 (etas catalog start date/time) for ETAS calculations. using catalog start, t_0 = t_now - catlen (3650.000000) = 2008-12-02 20:57:20.377673+00:00\n",
      "ETAS over etas_cat_range/xyz_range:  ([0, 11375], [612, 1225])\n",
      "etas_mpp worker xyz_range:  [1225, 1837]\n",
      "begin globalETAS.__init()__\n",
      "Overriding t_0 (etas catalog start date/time) for ETAS calculations. using catalog start, t_0 = t_now - catlen (3650.000000) = 2008-12-02 20:57:20.377673+00:00\n",
      "ETAS over etas_cat_range/xyz_range:  ([0, 11375], [1225, 1837])\n",
      "etas_mpp worker xyz_range:  [1837, 2450]\n",
      "begin globalETAS.__init()__\n",
      "Overriding t_0 (etas catalog start date/time) for ETAS calculations. using catalog start, t_0 = t_now - catlen (3650.000000) = 2008-12-02 20:57:20.377673+00:00\n",
      "ETAS over etas_cat_range/xyz_range:  ([0, 11375], [1837, 2450])\n",
      "etas_mpp worker xyz_range:  [2450, 3062]\n",
      "begin globalETAS.__init()__\n",
      "Overriding t_0 (etas catalog start date/time) for ETAS calculations. using catalog start, t_0 = t_now - catlen (3650.000000) = 2008-12-02 20:57:20.377673+00:00\n",
      "ETAS over etas_cat_range/xyz_range:  ([0, 11375], [2450, 3062])\n",
      "etas_mpp worker xyz_range:  [3062, 3675]\n",
      "begin globalETAS.__init()__\n",
      "Overriding t_0 (etas catalog start date/time) for ETAS calculations. using catalog start, t_0 = t_now - catlen (3650.000000) = 2008-12-02 20:57:20.377673+00:00\n",
      "ETAS over etas_cat_range/xyz_range:  ([0, 11375], [3062, 3675])\n",
      "etas_mpp worker xyz_range:  [3675, 4287]\n",
      "begin globalETAS.__init()__\n",
      "Overriding t_0 (etas catalog start date/time) for ETAS calculations. using catalog start, t_0 = t_now - catlen (3650.000000) = 2008-12-02 20:57:20.377673+00:00\n",
      "ETAS over etas_cat_range/xyz_range:  ([0, 11375], [3675, 4287])\n",
      "etas_mpp worker xyz_range:  [4287, 4900]\n",
      "begin globalETAS.__init()__\n",
      "Overriding t_0 (etas catalog start date/time) for ETAS calculations. using catalog start, t_0 = t_now - catlen (3650.000000) = 2008-12-02 20:57:20.377673+00:00\n",
      "ETAS over etas_cat_range/xyz_range:  ([0, 11375], [4287, 4900])\n",
      "begin make_etas_rtree()\n",
      "begin make_etas_rtree()\n",
      "len(local_lattice_dict):  613\n",
      "len(local_lattice_dict):  612\n",
      "begin make_etas_rtree()\n",
      "begin make_etas_rtree()\n",
      "len(local_lattice_dict):  612\n",
      "len(local_lattice_dict):  613\n",
      "begin make_etas_rtree()\n",
      "begin make_etas_rtree()\n",
      "begin make_etas_rtree()\n",
      "len(local_lattice_dict):  613\n",
      "len(local_lattice_dict):  612\n",
      "begin make_etas_rtree()\n",
      "len(local_lattice_dict):  612\n",
      "len(local_lattice_dict):  613\n",
      "Indices initiated. begin ETAS ::  [0, 11375]\n",
      "Indices initiated. begin ETAS ::  [0, 11375]\n",
      "now gather sub-arrays...\n",
      "Indices initiated. begin ETAS ::  [0, 11375]\n",
      "Indices initiated. begin ETAS ::  [0, 11375]\n",
      "Indices initiated. begin ETAS ::  [0, 11375]\n",
      "Indices initiated. begin ETAS ::  [0, 11375]\n",
      "Indices initiated. begin ETAS ::  [0, 11375]\n",
      "Indices initiated. begin ETAS ::  [0, 11375]\n",
      "finished calculateing ETAS (rtree). wrap up in recarray and return.\n",
      "etas complete (from mpp_rtree run() loop); now pipe back([0, 11375])\n",
      "finished calculateing ETAS (rtree). wrap up in recarray and return.\n",
      "etas complete (from mpp_rtree run() loop); now pipe back([0, 11375])\n",
      "finished calculateing ETAS (rtree). wrap up in recarray and return.\n",
      "etas complete (from mpp_rtree run() loop); now pipe back([0, 11375])\n",
      "finished calculateing ETAS (rtree). wrap up in recarray and return.\n",
      "etas complete (from mpp_rtree run() loop); now pipe back([0, 11375])\n",
      "finished calculateing ETAS (rtree). wrap up in recarray and return.\n",
      "etas complete (from mpp_rtree run() loop); now pipe back([0, 11375])\n",
      "finished calculateing ETAS (rtree). wrap up in recarray and return.\n",
      "etas complete (from mpp_rtree run() loop); now pipe back([0, 11375])\n"
     ]
    }
   ],
   "source": [
    "#etas = gep.ETAS_mpp(n_cpu=2*mpp.cpu_count(), catalog=nz_cat, **eq_prams)\n",
    "etas = gep.ETAS_mpp(n_cpu=2*mpp.cpu_count(), catalog=my_cat, **eq_prams)\n",
    "#\n",
    "print('*** DEBUG: ')\n",
    "print('{}, {}, {}, {}'.format(etas.t_now, etas.t_0, etas.t_forecast, mpd.num2date(etas.t_forecast)))\n",
    "\n",
    "# we've run this; we can reload it from pickle:\n",
    "#with open('data/etas_201610.pkl', 'rb') as fin:\n",
    "#    etas = pickle.load(fin)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(etas.t0, etas.t_now, etas.t_forecast, etas.catalog[-1])\n",
    "print(to_dt)\n",
    "print(etas.t_0, mpd.num2date(etas.t_forecast), etas.t_now, t_ms)\n",
    "#print(etas.catalog[-5:])\n",
    "#\n",
    "print('t_now: ', etas.t_now, max(etas.catalog['event_date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fg=plt.figure(0, figsize=(12,10))\n",
    "ax=plt.gca()\n",
    "etas.make_etas_contour_map(n_contours=25, fignum=0, map_resolution='f', alpha=.3, ax=ax)\n",
    "#\n",
    "#mainshock = sorted(etas.catalog, key=lambda rw: rw['mag'])[-1]\n",
    "#print('mainshock: ', mainshock)\n",
    "# get mainshock. it's an m>6 event in the last week or so... this is subjective.\n",
    "# if we just look for the biggest event, we get the L'Aquila event, so we'll need to be more creative...\n",
    "# or just specify it.\n",
    "\n",
    "#mainshock = etas.catalog[-1]\n",
    "mainshock = {'mag':7.9, 'lon':lon0, 'lat':lat0, 'event_date':dtm.datetime(2018,1,23,9, 31, 42, tzinfo=pytz.timezone('UTC'))}\n",
    "# 2017-11-12 18:18:17 UTC 34.886°N 45.941 W 23.2 km\n",
    "#\n",
    "for j,eq in enumerate(reversed(etas.catalog)):\n",
    "    #print('*** ', pytz.utc.localize(eq['event_date'].astype(dtm.datetime)))\n",
    "    if pytz.utc.localize(eq['event_date'].astype(dtm.datetime))<etas.t_now-dtm.timedelta(days=180): break\n",
    "    if eq['mag']>=mainshock['mag']:\n",
    "        mainshock = eq\n",
    "        #\n",
    "    #\n",
    "#\n",
    "print('ms: ', mainshock, mainshock['lon'], mainshock['lat'])\n",
    "x,y = etas.cm(mainshock['lon'], mainshock['lat'])\n",
    "\n",
    "#print('mm: ', max(etas.catalog['mag']))\n",
    "\n",
    "#\n",
    "# let's get everything m>6 in the last 6 months?\n",
    "m6s = [rw for rw in etas.catalog if rw['mag'] >= 5.5\n",
    "       and pytz.utc.localize(rw['event_date'].astype(dtm.datetime))>t_ms-dtm.timedelta(days=180)]\n",
    "\n",
    "# plot mainshock:\n",
    "dt = (mainshock['event_date'].astype(dtm.datetime) if isinstance(mainshock['event_date'], numpy.datetime64)\n",
    "      else mainshock['event_date'])\n",
    "#\n",
    "#dt=t0\n",
    "dt_str = '{}-{}-{} {}:{}:{}'.format(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second)\n",
    "#etas.cm.plot([x], [y], latlon=False, marker='*', color='r', ms=16, zorder=11,\n",
    "#                   label='m={}, {}'.format(mainshock['mag'], dt_str))\n",
    "etas.cm.plot([mainshock['lon']], [mainshock['lat']], latlon=False, marker='*', color='r', ms=16, zorder=11,\n",
    "                   label='m={}, *{}*'.format(m0, dt_str))\n",
    "\n",
    "ax.set_title('ETAS: {}, {}\\n\\n'.format(etas_name, etas.t_now), size=16)\n",
    "#for j,m6 in enumerate(m6s):\n",
    "for j,m6 in enumerate(sorted(sorted(m6s, key=lambda rw:rw['mag'])[:-1], key=lambda rw:rw['event_date'])):\n",
    "    clr = colors_[j%len(colors_)]\n",
    "    #\n",
    "    dt = m6['event_date'].astype(dtm.datetime)\n",
    "    dt_str = '{}-{}-{} {}:{}:{}'.format(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second)\n",
    "    etas.cm.scatter(m6['lon'], m6['lat'], s=3*(m6['mag']+12.), edgecolors=clr, latlon=False,\n",
    "                          c='none', marker='o', zorder=11, label='m={:.2f}, {}'.format(m6['mag'], dt_str))\n",
    "    #\n",
    "plt.gca().legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.clf()\n",
    "ax1 = plt.subplot('211')\n",
    "ax2 = plt.subplot('212', sharex=ax1)\n",
    "#ax1.plot(etas.catalog['event_date'], etas.catalog['mag'], marker='.', ls='-', label='mag')\n",
    "f_dates = [mpd.date2num(x.astype(dtm.datetime)) for x in etas.catalog['event_date']]\n",
    "ax1.vlines(f_dates, (min(etas.catalog['mag'])-.5)*numpy.ones(len(etas.catalog)), \n",
    "           etas.catalog['mag'], lw=2., label='mag', color='b')\n",
    "ax2.plot(f_dates[1:], numpy.diff(f_dates,1), '.-')\n",
    "\n",
    "ax2.set_ylabel('intervals $\\Delta t$')\n",
    "ax1.set_ylabel('magnitudes $m$')\n",
    "ax2.set_ylim(0, 1.25*sorted(numpy.diff(f_dates))[-2])\n",
    "\n",
    "print('max_date: {}'.format(etas.catalog['event_date'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fg=plt.figure(0, figsize=(12,10))\n",
    "ax=plt.gca()\n",
    "ax_mini = fg.add_axes([.65,.05, .15, .15])\n",
    "cmap='jet'\n",
    "etas.make_etas_contour_map(n_contours=25, fignum=0, map_resolution='i', alpha=.3, ax=ax, lats_map=etas.lats,\n",
    "                          lons_map=etas.lons, map_cmap=cmap)\n",
    "\n",
    "# etas.make_etas_contour_map(n_contours=25, fignum=0, map_resolution='l', alpha=.3, ax=ax_mini, lats_map=[-89., 89.],\n",
    "#                           lons_map=[-180., 180.], lat_interval=10., lon_interval=10.)\n",
    "\n",
    "ll_lon_mini = max(-180, lon0-50.)\n",
    "ll_lat_mini = max(-89, lat0-50)\n",
    "ur_lon_mini = min(180., lon0+50.)\n",
    "ur_lat_mini = min(89, lat0+50)\n",
    "mp_mini = Basemap(llcrnrlon=ll_lon_mini, llcrnrlat=ll_lat_mini,\n",
    "                                urcrnrlon=ur_lon_mini, urcrnrlat=ur_lat_mini,\n",
    "                  resolution='l', projection=etas.cm.projection, lon_0=0., lat_0=0., ax=ax_mini)\n",
    "mp_mini.drawcoastlines(color='gray', zorder=1)\n",
    "mp_mini.drawcountries(color='black', zorder=1)\n",
    "\n",
    "mp_mini.plot([x], [y], latlon=False, marker='*', color='r', ms=2, zorder=24)\n",
    "#\n",
    "X,Y = etas.cm(numpy.array(etas.lonses), numpy.array(etas.latses))\n",
    "Z = numpy.log10(etas.lattice_sites)\n",
    "#ax_mini.set_ylim(ll_lat_mini, ur_lat_mini)\n",
    "#ax_mini.set_xlim(ll_lon_mini, ur_lon_mini)\n",
    "#\n",
    "etas_contours = ax_mini.contourf(X,Y, Z, 15, zorder=8, alpha=.3, cmap=cmap)\n",
    "#\n",
    "# let's get everything m>6 in the last 6 months?\n",
    "m6s = [rw for rw in etas.catalog if rw['mag'] >= 5.5 \n",
    "       and pytz.utc.localize(rw['event_date'].astype(dtm.datetime))>to_dt-dtm.timedelta(days=120)]\n",
    "#\n",
    "# plot mainshock:\n",
    "dt = (mainshock['event_date'].astype(dtm.datetime) if isinstance(mainshock['event_date'], numpy.datetime64)\n",
    "      else mainshock['event_date'])\n",
    "dt_ms_str = '{}-{}-{} {}:{}:{}'.format(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second)\n",
    "etas.cm.plot([x], [y], latlon=False, marker='*', color='r', ms=16, zorder=11,\n",
    "                   label='m={:.2f}, {}'.format(mainshock['mag'], dt_ms_str))\n",
    "ax.set_title('ETAS: {}, {}\\n\\n'.format(etas_name, etas.t_now), size=16)\n",
    "#for j,m6 in enumerate(m6s):\n",
    "for j,m6 in enumerate(sorted(sorted(m6s, key=lambda rw:rw['mag'])[:-1], key=lambda rw:rw['event_date'])):\n",
    "    clr = colors_[j%len(colors_)]\n",
    "    #\n",
    "    dt = m6['event_date'].astype(dtm.datetime)\n",
    "    dt_str = '{}-{}-{} {}:{}:{}'.format(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second)\n",
    "    etas.cm.scatter(m6['lon'], m6['lat'], s=3*(m6['mag']+12.), edgecolors=clr, \n",
    "                          c='none', marker='o', zorder=11, label='m={:.2f}, {}'.format(m6['mag'], dt_str))\n",
    "    #\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(['{},{}\\n'.format(rw['event_date'], rw['mag'] ) for rw in etas.catalog if rw['mag'] >= 5.5])\n",
    "#len(['{},{}\\n'.format(rw['event_date'], rw['mag'] ) for rw in etas.catalog if rw['mag'] >= 5.5\n",
    "#     and rw['event_date'].astype(dtm.datetime)>dtm.datetime(2016,10,20)])\n",
    "\n",
    "print(dt, type(dt), dt.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for rw in m6s:\n",
    "    print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## hey, pickling works with the new ETAS objects (or at least it appears to...):\n",
    "# with open('data/etas_201610.pkl', 'wb') as fpkl:\n",
    "#     pickle.dump( etas, fpkl)    \n",
    "#\n",
    "#with open('data/etas_201610.pkl', 'rb') as fin:\n",
    "#    etas2 = pickle.load(fin)\n",
    "#\n",
    "#print(etas2.catalog[0:5])\n",
    "# TODO: we want the datetime part of the filename to come from the etas object itself, for purposes of\n",
    "# integrity. BUT, we want this script to be a bit more portable, so we should replace all the etas\n",
    "# references/object name to just 'etas'\n",
    "#\n",
    "etas.export_kml(os.path.join(f_path, '{}_{}.kml'.format(f_root, str(etas.t_now).replace(' ', '_'))), \n",
    "                kml_contours_bottom=.25, kml_contours_top=1.0)\n",
    "\n",
    "etas.export_xyz(os.path.join(f_path, '{}_{}.xyz'.format(f_root, str(etas.t_now).replace(' ', '_'))))\n",
    "fg.savefig(os.path.join(f_path, '{}_{}.png'.format(f_root, str(etas.t_now).replace(' ', '_'))))\n",
    "\n",
    "with open (os.path.join(f_path, '{}_{}.pkl'.format(f_root, str(etas.t_now).replace(' ', '_'))), 'wb') as fpkl:\n",
    "    pickle.dump(etas, fpkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.clf()\n",
    "ax1=plt.subplot('121')\n",
    "ax2=plt.subplot('122')\n",
    "ax1.plot(etas.catalog['lon'], etas.catalog['lat'], ',')\n",
    "ax1.plot([mainshock['lon']], [mainshock['lat']], marker='*', color='r', ms=16, zorder=11)\n",
    "#\n",
    "ax2.plot([m for m in reversed(sorted(etas.catalog['mag']))], numpy.arange(1,len(etas.catalog)+1),\n",
    "         '.-', lw=2.)\n",
    "ax2.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(etas.mc, etas.mc_etas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ETAS cross-sections:\n",
    "- What is the best way to produce an ETAS cross section?\n",
    "#### PCA and coordinate rotation method(s):\n",
    "- Start with a more or less contemporary method; compute (global) PCA; rotate along the major axis; cross section is the median \"x'\" row.\n",
    "  - Variations: mean-subtract via (un-) weighted or custom location (to specify the center of rotation)\n",
    "  - ???\n",
    "##### Comment on this method:\n",
    "- Straight forward(ish) in principle, and fairly conventional in the Physics community, but potentially compute intensive, and clumsy to customize or do anything but straight lines.\n",
    "- Also, this breaks the grid-symmetry of the lattice (it rotates the whole lattice), so plotting, etc. of the rotated frame would require reconstruction onto a new lattice.\n",
    "- Even if we don't want to do gridded plots, it moves points off of the original lattice, for whatever that's worth.\n",
    "\n",
    "### TODO items:\n",
    "\n",
    "####\n",
    "Add bounds selection:\n",
    "- We've started with PCA over the full map domain, assuming the 'mainshock' dominates/characterizes the space.\n",
    "- Obviously, this will not always be the case, so add options to select a lat/lon subdomain.\n",
    "- Then, we move on to the generalized NN-interpoladed cross section\n",
    "\n",
    "#### NN to any user defined path:\n",
    "- Designate a path -- it can be a straight line.\n",
    "- Use an *sklearn* NN finding algorithm to find the closest $n$ points in the ETAS map to each point in the path (we will need to define our path to have sufficient resolution)\n",
    "- Compute the z-value for each point on the path from a mean (presumably weighted by distance) between each point on the path and its NN values.\n",
    "- Note: we could generalize this to use the entie map for each point... which might be interesting as well.\n",
    "- Note: we'll need some sort of Omori like weighting, to handle singularities, but this can be more or less arbitrarily chosen.... and it's just the weighting, so it should not have huge effect in most cases.\n",
    "- **Note possible application:** We might use something like this to stitch together smaller earthquakes (ETAS halos) to estimate rupture-hazard zones for larger events (aka, stitch togetehr hot-spots that are more or less in a line, or consistent with a geophysical profile (regional faults) of some sort).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot a cross-section of ETAS. let's get smart about this and do a PCA on the xyz field...\n",
    "#XYw = numpy.array([[x*z, y*z] for x,y,z in etas.ETAS_array])\n",
    "class PCA_cross_section(list):\n",
    "    def __init__(self, XYW, x_min=None, x_max=None, y_min=None, y_max=None, n_NN=4, n_points_xc=None):\n",
    "        # compute the covariance of [[x*w, y*w], ...], get eigen-value/vectors,\n",
    "        #  construct a cross section vector, then compute cross-section values via a weighted\n",
    "        #  average (which we can show is a Bayes maximum-likelihood value) from each point's NN.\n",
    "        # TODO: add an option for a distance calculation? use a spherical distance formula in\n",
    "        #  both the NN finder (presumably from sklearn, which i think uses an r-tree index) and\n",
    "        #  the weighted average.\n",
    "        #\n",
    "        # TODO: do we need to keep the original inputs? this is more memory and compute intensive. how often\n",
    "        #  do we re-use these objects? are we typically better off just recomputing the whole class for variations\n",
    "        #  on PCA bounds, etc?\n",
    "        XYW = numpy.array(XYW)\n",
    "        #\n",
    "        if x_min is None: x_min = min(XYW.T[0])\n",
    "        if x_max is None: x_max = max(XYW.T[0])\n",
    "        if y_min is None: y_min = min(XYW.T[1])\n",
    "        if y_max is None: y_max = max(XYW.T[1])\n",
    "        #\n",
    "        #XYW_pca = XYW[XYW.T[0]>=x_min and XYW.T[0]<=x_max and XYW.T[1]>=y_min and XYW.T[1]<=y_max]\n",
    "        f_between = lambda x, y, x1, x2, y1, y2: (x>=x1 and x<=x2 and y>=y1 and y<=y2)\n",
    "        #\n",
    "        # trying to use numpy indexing here, so we end up keeping the indices of the array where these\n",
    "        #  criteria are met. having trouble getting it to take a multi-valued condition. maybe the better\n",
    "        #  approach is to pass an array of indices that satisfy the \"between\" condition?\n",
    "        #\n",
    "        # this requires multiple passes throught the array (which is most likely not terribly costly)\n",
    "        #XYW_pca = XYW[XYW.T[0]>=x_min]\n",
    "        #XYW_pca = XYW_pca[XYW.T[0]<=x_max]\n",
    "        # TODO: maybe, instead of copying this, we define the index and implement it as a @property function?\n",
    "        # so idx_pca = numpy.array([k for k,(x,y,w) in... ]) and then self.XYW_pca \n",
    "        # returns self.XYW[self.idx_pca] ??\n",
    "        XYW_pca = XYW[numpy.array([k for k,(x,y,w) in enumerate(XYW) \n",
    "                                   if f_between(x,y, x_min, x_max, y_min, y_max)])]\n",
    "        #\n",
    "        #print('***DEBUG lens: ', len(XYW_pca), len(XYW))\n",
    "        #n_points_xc = n_points_xc or len(XYW_pca)\n",
    "        n_points_xc = n_points_xc or max(len(set(XYW_pca.T[0])), len(XYW_pca))\n",
    "        #\n",
    "        #XYw_pca = numpy.array([[x*w,y*w] for x,y,w in XYW_pca ])\n",
    "        # TODO?: interpolate Y,w onto a regulaized X axis, or assume valid inputs?\n",
    "        #\n",
    "        #w_cov = numpy.cov(XYw_pca, rowvar=False)\n",
    "        # TODO: will matrices be properly aligned if we just skip all of the A.T ? probably at least\n",
    "        #      one layer of this to revise...\n",
    "        #\n",
    "        # as a sanity check, do a line-fit to the local data to get an approximate b-value:\n",
    "        xy_w = XYW_pca.T[0:2].T*numpy.atleast_2d(XYW_pca.T[2]).T\n",
    "        lsq_xyz = numpy.linalg.lstsq([[1.,x] for x,y in xy_w], [y for x,y in xy_w])\n",
    "        #print('** DEBUG lsq: ', lsq_xyz[0])\n",
    "        #\n",
    "        w_cov = numpy.cov(XYW_pca.T[0:2].T*numpy.atleast_2d(XYW_pca.T[2]).T, rowvar=False)\n",
    "        # Note: leave eig_vecs matrix intact, so we can use it for rotation transformations.\n",
    "        eig_vals, eig_vecs = numpy.linalg.eig(w_cov)\n",
    "        #print('*** Debug (prelim): ', eig_vals, eig_vecs)\n",
    "        #\n",
    "        # now, sort by eigenvalues:\n",
    "        idx = (eig_vals**2.).argsort()[::-1]   \n",
    "        #\n",
    "        #print('eigs (idx, vals, vecs): ', idx, len(eig_vals), len(eig_vecs))\n",
    "        #\n",
    "        #eig_vals = eig_vals.T[idx].T\n",
    "        # TODO: sort out (equivalent) syntax for (not) fancy inxing... this is from SourceForge, or\n",
    "        #    or something, but it is not quite right (i never use this syntax, but it's probably fast)\n",
    "        #e1,e2 = eig_vecs[:,idx]\n",
    "        # this looks correct (ish):\n",
    "        e1, e2 = eig_vecs.T[idx]\n",
    "        # note: we need to be careful about how we define the vectors of the data vs the axes. are we\n",
    "        #   rotating the axes to the data or the data to the axes (which are identical/inverse) operations\n",
    "        #   but it is important to be clear with respect too making rotations vs drawing cross-secgtions.\n",
    "        #\n",
    "        #print('*** e1, e2: ', e1, e2)\n",
    "        #print('eigs (idx, vals, vecs): ', idx, len(eig_vals), len(eig_vecs))\n",
    "        #\n",
    "        # Compute linear slope factors, for y' = a + bx type transformations, as opposed to the\n",
    "        #  X' = dot(e_k, X) or X' = L_x*e_1, y' = L_y * e_2 approaches (just multiplying the eigen-vectors).\n",
    "        b_major = e1[1]/e1[0] \n",
    "        b_minor = e2[1]/e2[0]\n",
    "        #\n",
    "        # stash inputs:\n",
    "        self.__dict__.update({key:val for key,val in locals().items() if not key in ('self', '__class__')})\n",
    "        #\n",
    "        # etas.ETAS_array['x'], y0 + b_major*(etas.ETAS_array['x'] - x0)\n",
    "        X_pca, Y_pca, W_pca = XYW_pca.T\n",
    "        X = numpy.array(sorted(set(X_pca)))\n",
    "        # XY = [pca_cross_2.e1*x for x in numpy.linspace(-5., 5., 100)]\n",
    "        #\n",
    "        dx = max(X_pca)-min(X_pca)\n",
    "        x0 = numpy.mean(X_pca)\n",
    "        y0 = numpy.mean(Y_pca)\n",
    "        Xs = numpy.linspace(-dx, dx, n_points_xc)\n",
    "        # numpy.mean(X_pca)\n",
    "        #\n",
    "        # TODO: maybe revisit the default cross-section vector.\n",
    "        super(PCA_cross_section, self).__init__(numpy.array([X, \n",
    "                                                    numpy.mean(Y_pca) + b_major*(X-numpy.mean(X_pca))]).T)\n",
    "        #y0 = numpy.mean(Y_pca)\n",
    "        #\n",
    "        # probably the 'right' way to compute the cross-section vector is to just multiply (scale) and \n",
    "        #  translate (add to) the eigen-vector:\n",
    "        #super(PCA_cross_section, self).__init__([[e1[0]*x + x0, e1[1]*x + y0]\n",
    "        #                                for x in numpy.linspace(-dx, dx, n_points_xc)])\n",
    "        del X_pca, Y_pca, W_pca, X\n",
    "        #\n",
    "        \n",
    "    #\n",
    "    def get_cross_section_xy(self, x_min=None, x_max=None, y_min=None, y_max=None, n_points=250, b=None):\n",
    "        '''\n",
    "        # TODO: this should work -- it will return a  cross-section, but i think we need to be more thoughtful\n",
    "        #     about how we go about this, specifically how we choose the middle. we'll need some variationos\n",
    "        #     of this to use the center, weighted center, etc. and then a smart way to draw the cross-section\n",
    "        #     axis through it (y = a + bx vs (x',y') = x*v1)\n",
    "        #\n",
    "        '''\n",
    "        if b is None: b = self.b_major\n",
    "        #\n",
    "        if x_min is None:\n",
    "            x_min = min(self.X)\n",
    "        if x_max is None:\n",
    "            x_max = max(self.X)\n",
    "        if y_min is None:\n",
    "            y_min = min(self.Y)\n",
    "        if y_max is None:\n",
    "            y_max = max(self.Y)\n",
    "        #\n",
    "        # \n",
    "        X = numpy.linspace(x_min, x_max, n_points)\n",
    "        #return numpy.array([X, numpy.mean(Y_pca) + b*(X-numpy.mean(self.X_pca))]).T\n",
    "        #\n",
    "        # this will use an unweighted mean to center:\n",
    "        #return numpy.array([[x,y] for x,y in \n",
    "        #                numpy.array([X, numpy.mean(self.Y_pca) + b*(X-numpy.mean(self.X_pca))]).T\n",
    "        #                    if (y>=y_min and y<=y_max)])\n",
    "        #\n",
    "        # ... and a weighted mean:\n",
    "        return numpy.array([[x,y] for x,y in \n",
    "                        numpy.array([X, numpy.average(self.Y_pca, weights=self.W_pca) +\n",
    "                                     b*(X-numpy.average(self.X_pca, weights=self.W_pca))]).T\n",
    "                            if (y>=y_min and y<=y_max)])\n",
    "\n",
    "    def get_cross_section_zs(self, XY_xc=None, XYZ=None, n_NN=None):\n",
    "        #TODO: this is not working. so maybe take it off-line to work out the code, then put back into\n",
    "        #    the class.\n",
    "        #\n",
    "        n_NN = n_NN or self.n_NN\n",
    "        if XY_xc is None: XY_xc = numpy.array(self)\n",
    "        #if XYZ is None:   XYZ = self.XYW_pca\n",
    "        if XYZ is None:   XYZ = self.XYW\n",
    "        #\n",
    "        # get NN:\n",
    "        # TODO: look at \"fancy\" indexing version of this X.T[0:2].T operation, something like:\n",
    "        #   X[0:2, :]\n",
    "        nbrs = sklearn.neighbors.NearestNeighbors(n_neighbors=n_NN,\n",
    "                                                algorithm='ball_tree').fit(XYZ.T[0:2].T)\n",
    "        #\n",
    "        # TODO: i think a better and more efficient way to do this is to just assign the weights, from\n",
    "        #  the distances, as w_jk = 1/(r_jk + <r>_k) or 1.\n",
    "        #  where \"or\" kicks in if the denominator (r_jk + <r>_k) == 0\n",
    "        # we will want to carefully evaluate the corner cases and evaluate how the fraction addition\n",
    "        # is affected when we have these singular cases.\n",
    "        \n",
    "        distances, indices = nbrs.kneighbors(XY_xc)\n",
    "        #\n",
    "        # TODO: are distances always positive?\n",
    "        mean_distances = numpy.mean(distances, axis=1)\n",
    "        #denom = numpy.array([1./r if r!=0 else 1. for r in mean_distances])\n",
    "        weights        = numpy.array([[1./(r+mu) if (r+mu)!=0. else 1. for r in rw]\n",
    "                                      for rw, mu in zip(distances, mean_distances)])\n",
    "        \n",
    "        #\n",
    "        # we want a weighted average of the z values, based on NN distances,\n",
    "        # z_xc_k = sum_j(z_j/r_jk)/sum(1/r_jk)\n",
    "        # but we need to handle 1/0 cases in a generalized way. maybe something like:\n",
    "        # w = 1/(r_jk + a*<r>_k)\n",
    "        # where <r>_k is the mean nn distance and a is a tuning parameter;\n",
    "        #  if <r>==0, evenly weight all elements?\n",
    "        #\n",
    "        # like this:\n",
    "        # (but the main thing is to efficiently handle the x/0 cases).\n",
    "        #\n",
    "        z_xc = [numpy.dot(XYZ.T[2][js], ws)/numpy.sum(ws)\n",
    "                for js, ws, mu in zip(indices, weights, mean_distances)]\n",
    "        #z_xc = [numpy.sum([XYZ[j][2] for j in js]) for js, ws, mu in zip(indices, weights, mean_distances)]\n",
    "        #\n",
    "        return z_xc\n",
    "        \n",
    "    #@property\n",
    "    #def XYW_pca(self):\n",
    "    #    numpy.array([[x,y,w] for x,y,w in XYw if (x>=x_min and x<=x_max and y>=y_min and y<=y_max) ])\n",
    "    #\n",
    "    @property\n",
    "    def X(self):\n",
    "        return self.XYW.T[0]\n",
    "    @property\n",
    "    def Y(self):\n",
    "        return self.XYW.T[1]\n",
    "    @property\n",
    "    def w(self):\n",
    "        return self.XYW.T[2]\n",
    "    #\n",
    "    @property\n",
    "    def X_pca(self):\n",
    "        return self.XYW_pca.T[0]\n",
    "    @property\n",
    "    def Y_pca(self):\n",
    "        return self.XYW_pca.T[1]\n",
    "    @property\n",
    "    def W_pca(self):\n",
    "        return self.XYW_pca.T[2]\n",
    "    #\n",
    "    \n",
    "    \n",
    "    \n",
    "pca_cross = PCA_cross_section(XYW = numpy.array([etas.ETAS_array['x'], etas.ETAS_array['y'], \n",
    "                                        numpy.log(etas.ETAS_array['z'])]).T,\n",
    "                             x_min=-148, x_max=-145, y_min=61., y_max=None, n_NN=4, )\n",
    "pca_cross_2 = PCA_cross_section(XYW = numpy.array([etas.ETAS_array['x'], etas.ETAS_array['y'],\n",
    "                                        numpy.log(etas.ETAS_array['z'])]).T,\n",
    "                             x_min=-152., x_max=-146., y_min=53., y_max=59., n_NN=4, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-section examples\n",
    "- First, let's have a look at the pca_cross object's eigen-vector basis\n",
    "- Illustrate a couple of ways to access those vectors as basis vectors, a transformation matrix, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(0.35195595/(-0.93601656))\n",
    "print(0.35195595**2. + (-0.93601656)**2.)\n",
    "\n",
    "fg = plt.figure()\n",
    "ax=plt.gca()\n",
    "#\n",
    "ax.plot(*zip([0.,0.], pca_cross.eig_vecs.T[0]), color='b', ls='-', marker='o')\n",
    "ax.plot(*zip([0.,0.], pca_cross.eig_vecs.T[1]), color='r', ls='-', marker='s')\n",
    "#ax.plot(range(2), [0., 0.], color='k', alpha=.5, ls='-')\n",
    "ax.grid()\n",
    "ax.set_ylim(-1., 1.)\n",
    "ax.set_xlim(-1., 1.)\n",
    "\n",
    "\n",
    "fg = plt.figure()\n",
    "ax=plt.gca()\n",
    "#\n",
    "# ax.plot([0., pca_cross.eig_vecs[0][0]], [0., pca_cross.eig_vecs[1][0]], color='b', ls='-', marker='o')\n",
    "# ax.plot([0., pca_cross.eig_vecs[0][1]], [0., pca_cross.eig_vecs[1][1]], color='b', ls='-', marker='o')\n",
    "\n",
    "\n",
    "#ax.plot([0., pca_cross.e1[0]], [0., pca_cross.e1[1]], color='r', ls='-', marker='o')\n",
    "ax.plot(*zip([0., 0.], pca_cross.e2), color='r', ls='-', marker='o', label='e2')\n",
    "ax.plot(*zip([0., 0.], pca_cross.e1), color='b', ls='-', marker='o', label='e1')\n",
    "ax.set_title('eig_vals: {}'.format(pca_cross.eig_vals))\n",
    "ax.grid()\n",
    "ax.set_ylim(-1., 1.)\n",
    "ax.set_xlim(-1., 1.)\n",
    "ax.set_title('pca_cross')\n",
    "ax.legend(loc=0)\n",
    "#\n",
    "print('eig_vecs.T: ', list(zip([0.,0], pca_cross.eig_vecs.T[0])))\n",
    "print('dot_vecs: ', numpy.dot(pca_cross.eig_vecs.T[0], pca_cross.eig_vecs.T[1]))\n",
    "print('dot_e: ', numpy.dot(pca_cross.e1, pca_cross.e2))\n",
    "print('dot_e: ', numpy.dot(pca_cross_2.e1, pca_cross_2.e2))\n",
    "\n",
    "print('e1, e2: ', pca_cross.e1, pca_cross.e2)\n",
    "print('e1, e2: ', pca_cross_2.e1, pca_cross_2.e2)\n",
    "\n",
    "#\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "ax = plt.gca()\n",
    "XY = [pca_cross_2.e1*x for x in numpy.linspace(-5., 5., 100)]\n",
    "ax.plot([0., 0.], [0., 1.], color='k', alpha=.8)\n",
    "ax.plot([0., 1.], [0., 0.], color='k', alpha=.8)\n",
    "\n",
    "ax.plot(*zip(*XY), ls='-', marker='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Transform \n",
    "- Get a cross-section via PCA transformation (rotation to principal axes basis)\n",
    "- Note: We plot the original image using impshow(), but the rotated image is 1) no longer square, and 2) no longer on a regular lattice, so we have to do some refactoring, namely to rotate the image into a square frame and interpolate it onto a regular (pixelated) lattice.\n",
    "- We show what happens when this is not done\n",
    "- We show a quick-n-easy scatter() plot method to plot the rotated image without doing this. This method is scale dependent (aka, it will be sparse if the image is big and blurry/blotchy if the image is small) because the 'scatter' pixels are of fixed size. There are smarter ways to do this, but right now we're just using it to eye-ball the rotation.\n",
    "- We can then grab the median row from the rotated image to get a cross-section. This is less trivial than it sounds, since in order to really do this, we need to snap the pixels to a lattice or come up with some other criteria for continuity.\n",
    "- In the end, we revise this basic approach by externally defining a sequence along the principal axis, of some arbitrary resolution, and computing the mean() (or some other aggregate) z-value based from the N nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XY_prime = etas.ETAS_array.copy()\n",
    "XY_prime['x'] -= numpy.mean(XY_prime['x'])\n",
    "XY_prime['y'] -= numpy.mean(XY_prime['y'])\n",
    "XY_prime['z'] = numpy.log(XY_prime['z'])\n",
    "#\n",
    "X  = sorted(set(XY_prime['x']))\n",
    "dx = numpy.mean(numpy.diff(X))\n",
    "Y  = sorted(set(XY_prime['y']))\n",
    "dy = numpy.mean(numpy.diff(Y))\n",
    "#\n",
    "#XY_prime['x'], XY_prime['y'] = numpy.dot(eig_vecs, numpy.array([XY_prime['x'], XY_prime['y']]))\n",
    "#XY_prime = sorted(list(XY_prime), key=lambda rw: (rw[0], rw[1]))\n",
    "#\n",
    "#print('lens: ', len(XY_prime), len(XYw))\n",
    "plt.figure(figsize=(8,6))\n",
    "ax1 = plt.subplot('121')\n",
    "ax2 = plt.subplot('122')\n",
    "ax1.imshow(numpy.reshape(XY_prime['z'], (len(Y), len(X))), origin='lower')\n",
    "#\n",
    "XY_prime['x'], XY_prime['y'] = numpy.dot(pca_cross.eig_vecs, numpy.array([XY_prime['x'], XY_prime['y']]))\n",
    "#\n",
    "# Snap to lattice:\n",
    "# now, we need to 'snap' (x,y) to a regular lattice. a few options for this, like x' = x - x%dx\n",
    "#   but we nominally want to snap to the closest point... we also need to fill in the lattice, so this is\n",
    "#   not super simple if we want to do contours, imshow(), etc. but to just get data... we can do this.\n",
    "#\n",
    "XY_prime['x'] = dx*numpy.round(XY_prime['x']/dx)\n",
    "XY_prime['y'] = dy*numpy.round(XY_prime['y']/dy)\n",
    "#\n",
    "print('dx, dy: ', dx, dy)\n",
    "#XY_prime = sorted([[x,y,z] for x,y,z in XY_prime], key=lambda rw: (rw[0], rw[1]))\n",
    "#\n",
    "XY_prime.sort(order = ('y', 'x'))\n",
    "#\n",
    "ax2.imshow(numpy.reshape([z for x,y,z in XY_prime], (len(X), len(Y))), origin='lower')\n",
    "#ax2.contourf(XY_prime['x'], XY_prime['y'], XY_prime['z'], N=15)\n",
    "#\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.clf()\n",
    "plt.suptitle('scatter plots')\n",
    "ax1 = plt.subplot('121')\n",
    "ax2 = plt.subplot('122')\n",
    "ax2.scatter(x=XY_prime['x'], y=XY_prime['y'], c=XY_prime['z'], cmap='jet')\n",
    "ax2.plot([min(XY_prime['x']), max(XY_prime['x'])], [0.,0.], ls='--', marker='', zorder=11, color='k')\n",
    "ax1.scatter(x=etas.ETAS_array['x'], y=etas.ETAS_array['y'], c=numpy.log(etas.ETAS_array['z']),\n",
    "            cmap='jet')\n",
    "#\n",
    "x0 = numpy.mean(etas.ETAS_array['x'])\n",
    "y0 = numpy.mean(etas.ETAS_array['y'])\n",
    "#\n",
    "b_major = pca_cross_2.eig_vecs.T[1][1]/pca_cross_2.eig_vecs.T[1][0]\n",
    "#\n",
    "# TODO: reconsile this transformation; i think this local calculation is wrong:\n",
    "#ax1.plot(etas.ETAS_array['x'], y0 + b_major*(etas.ETAS_array['x'] - x0), color='k', ls='-', lw=2., zorder=11)\n",
    "ax1.plot(*zip(*pca_cross), color='r', ls='--', lw=2., zorder=12)\n",
    "ax1.plot(*zip(*pca_cross_2), color='m', ls='--', lw=2., zorder=12)\n",
    "#\n",
    "ax1.plot(*zip(*pca_cross_2.get_cross_section_xy(b=pca_cross_2.b_minor, \n",
    "                y_min=min(pca_cross_2.Y_pca), y_max=max(pca_cross_2.Y_pca))),\n",
    "         ls='--', lw=2., color='b', zorder=12)\n",
    "#\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "#\n",
    "# now, get the median row, along the x' axis. note: we want to be careful to get an actual y-value, not\n",
    "#  an intermediate, and the data are no longer properly gridded... but they are snapped to a grid.\n",
    "y_med = sorted(XY_prime['y'])[int(.5*len(XY_prime))]\n",
    "x_section = [[x,z] for x,y,z in XY_prime if y==y_med]\n",
    "plt.figure()\n",
    "ax=plt.gca()\n",
    "ax.plot(*zip(*x_section), ls='-', marker='')\n",
    "# plt.figure()\n",
    "# ax=plt.gca()\n",
    "# ax.plot([z for x,y,z in XY_prime if y==y_med], ls='-', marker='')\n",
    "#\n",
    "#\n",
    "#plt.figure()\n",
    "##my_xy = [[x,y,z] for x,y in zip(etas.catalog['lon'], etas['lat']) if x>=42.25 and x<=42.75]\n",
    "#my_xyz = [[x,y,z] for x,y,z in etas.ETAS_array if y>=42.25 and y<=42.75]\n",
    "#plt.figure(figsize=(8,6))\n",
    "#plt.plot([x for x,y,z in my_xyz ], [z for x,y,z in my_xyz], '.')\n",
    "\n",
    "#etas.make_etas_contour_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN based cross section:\n",
    "- A better (??) approach to finding teh cross-section is to define the geospatial cross-section trace vector and then aggregate the $N_{nn}$ nearest-neighbor values to that point. Note that the cross-section trace need not lie on the image lattice.\n",
    "- Note that this approach facilitates some interesting interpretations of this metric. For exmple, we might evaluate the cross-section trace based on the $w \\propto 1/r^q$ weighted average of all points in the lattice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "my_vec = pca_cross_2.get_cross_section_xy()\n",
    "#b=pca_cross_2.b_minor, \n",
    "#                y_min=min(pca_cross_2.Y_pca), y_max=max(pca_cross_2.Y_pca))\n",
    "zs = pca_cross_2.get_cross_section_zs(my_vec, n_NN=4)\n",
    "\n",
    "print('lens: ', len(my_vec), len(zs))\n",
    "xy_mean = numpy.average(pca_cross.XYW.T[0:2].T, weights=pca_cross.XYW.T[2], axis=0)\n",
    "xy_cross = xy_mean + numpy.array([pca_cross_2.e2*x for x in numpy.linspace(-3.,3.,1000)])\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(zs)\n",
    "ax.plot(pca_cross_2.get_cross_section_zs(pca_cross_2.get_cross_section_xy(b=pca_cross_2.b_minor)))\n",
    "ax.plot(pca_cross.get_cross_section_zs(xy_cross))\n",
    "\n",
    "ax.plot(pca_cross.get_cross_section_zs(xy_cross, n_NN=10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mycat = atp.catfromANSS(lon=lons, lat=lats, minMag=2.5,\n",
    "                        dates0=[dtm.datetime(2005,1,1, tzinfo=tzutc), None], Nmax=None, fout=None, rec_array=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: take a look at the ComCat library, which should combine multiple catalogs and may provide\n",
    "#  tensor solution information, like strike, etc.\n",
    "#\n",
    "test_cat = atp.catfromANSS(lat=lats, lon=lons, minMag=1.5,\n",
    "                           dates0=[dtm.datetime(1990,1,1, tzinfo=pytz.utc), dtm.datetime.now(pytz.utc)])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "#plt.plot(test_cat['lon'], test_cat['lat'], '.')\n",
    "print(test_cat[-5:])\n",
    "print(max(test_cat['event_date']))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.clf()\n",
    "ax1=plt.subplot('121')\n",
    "ax2=plt.subplot('122')\n",
    "ax1.plot(test_cat['lon'], test_cat['lat'], ',')\n",
    "ax1.plot([mainshock['lon']], [mainshock['lat']], marker='*', color='r', ms=16, zorder=11)\n",
    "#\n",
    "ax2.plot([m for m in reversed(sorted(test_cat['mag']))], numpy.arange(1,len(test_cat)+1),\n",
    "         '.-', lw=2.)\n",
    "ax2.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.clf()\n",
    "ax=plt.gca()\n",
    "n=1500\n",
    "ax.plot(test_cat['event_date'][-n:], test_cat['mag'][-n:], '.-')\n",
    "ax.plot(etas.catalog['event_date'][-n:], etas.catalog['mag'][-n:], '.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_cat[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.gca()\n",
    "\n",
    "def f_omori(t, tau=1., t0=1., p=1.):\n",
    "    return 1./(tau*(t0 + t)**p)\n",
    "\n",
    "X = numpy.linspace(0., 10., 1000)\n",
    "\n",
    "ax.plot(X, f_omori(X, p=1.00), ls='-', marker='', lw=3., label='$p=1, t0=1, \\\\tau=1$')\n",
    "#ax.plot(X, f_omori(X, p=1.5), ls='-', marker='', lw=3., label='$p=1.5$')\n",
    "\n",
    "ax.plot(X, f_omori(X, t0=1.5, p=1.00), ls='-', marker='', lw=3., label='$p=1$, t_0=1.5, \\\\tau=1.0')\n",
    "ax.plot(X, f_omori(X, t0=.5, p=1.00), ls='-', marker='', lw=3., label='$p=1$, t_0=0.5, \\\\tau=1.0')\n",
    "\n",
    "ax.plot(X, f_omori(X, p=1.00, tau=1.5), ls='-', marker='', lw=3., label='$p=1$, t_0=0.5, \\\\tau=1.5')\n",
    "ax.plot(X, f_omori(X, p=1.00, tau=.5), ls='-', marker='', lw=3., label='$p=1$, t_0=1.0, \\\\tau=0.5')\n",
    "\n",
    "#\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('time $t$', size=18)\n",
    "ax.set_ylabel('Rate or Density $\\\\rho$', size=18)\n",
    "ax.set_title('An Omori-like modified power law')\n",
    "ax.legend(loc=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "ax = plt.gca()\n",
    "X = numpy.array([x.astype(float) for x in etas.catalog['event_date']])\n",
    "X -= min(X)\n",
    "n = 20\n",
    "Y = numpy.array([x2 - x1 for x1,x2 in zip(X[0:-n], X[n:]) ])\n",
    "\n",
    "ax.plot(X[n:], Y)\n",
    "#ax.set_yscale('log')\n",
    "\n",
    "print(etas.catalog.dtype)\n",
    "print(etas.catalog['event_date'][0:5])\n",
    "type(etas.catalog['event_date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
